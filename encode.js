var Int64 = require('node-int64')
var varint = require('varint')
//encode an empty parquet file.
//should be like this:
/*
PAR1
<FileMetadata>
<length(FileMetaData)>
PAR1
*/
var BufferList = require('bl')

var thrift = require('thrift')
var types = require('./gen-nodejs/parquet_types')

function plain(value) {
  var v = new Buffer(value)
  var len = new Buffer(4)
  len.writeUInt32LE(v.length, 0)
  return Buffer.concat([len, v])
}

//value must be a thrift type.
function encode(value) {
  var output = []
  var transport = new thrift.TBufferedTransport(null, function (buf) {
    output.push(buf)
  })
  var protocol = new thrift.TCompactProtocol(transport)
  value.write(protocol)
  transport.flush()
  return Buffer.concat(output)
}

function encodeRepeats(repeats, value) {
  var len = varint.encodingLength(repeats << 1)
  var b = new Buffer(4 + len + 1)
  b.writeUInt32LE(len+1, 0)
  varint.encode(repeats << 1, b, 4)
  b[4 + len] = value
  return b
}

function encodeColumn(name, column) {
  var count = column.length
  var values =
    Buffer.concat([
      //these 6 bytes are actually a hybrid RLE, it seems of the repetition level?
      //the column starts with a hybrid-rle/bitpack of the definition
      //level. for a flat schema with all fields, that is the
      //same as a lot of 1's. that can be encoded most compactly
      //as a RLE.

      //Question: how is the bitwidth of the RLE calculated?
      //I'm guessing it's something in the schema?
      encodeRepeats(column.length, 1)
    ].concat(column.map(plain)))

  var ph = new types.PageHeader()

  ph.type = '0'
  ph.uncompressed_page_size = values.length
  ph.compressed_page_size = values.length
  ph.crc = null
  ph.data_page_header = new types.DataPageHeader()
  ph.data_page_header.num_values = count
  ph.data_page_header.encoding = '0'   //plain encoding
  ph.data_page_header.definition_level_encoding = 3 //3 //RLE encoding
  ph.data_page_header.repetition_level_encoding = 4 //Bitpacked encoding
  //statistics is optional, but leaving it off probably slows
  //some queries.
  //ph.data_page_header.statistics

  var data_page = Buffer.concat([
    //unfortunately, the page header
    //is expected before the values
    //which means we can't stream the values
    //then write the header...
    //but I guess the idea is to write a column_chunk at a time
    //(with a page_header at the top)
    encode(ph),
    values
  ])

  return data_page
}

module.exports = function (headers, table) {
  var PAR1 = new Buffer("PAR1")
  var buf = new BufferList()
  buf.append(PAR1)

  var count = table.length

  var fmd = new types.FileMetaData()
  var _schema = new types.SchemaElement()
  _schema.name = 'hive_schema'
  _schema.num_children = headers.length

  var schemas = headers.map(function (name) {

    var schema = new types.SchemaElement()
    schema.name = name
    schema.type = 6 //binary aka string
    schema.repetition_type = 1
    //note, javascript code generated by thrift does not check
    //falsey values correctly, but parquet uses an old version of thrift
    //so it's easier to set it like this.
    schema.converted_type = '0'
    return schema
  })

  var columns = []

  table.forEach(function (row) {
    row.forEach(function (value, i) {
      columns[i] = columns[i] || []
      columns[i].push(value)
    })
  })

  var column_chunks = headers.map(function (name, i) {
    var data_page = encodeColumn(name, columns[i])
    var column = new types.ColumnChunk()
    var metadata = new types.ColumnMetaData()

    column.file_offset = new Int64(buf.length) 
    column.meta_data = metadata
    var start = buf.length
    buf.append(data_page) //APPEND this column.

    metadata.type = 6
    metadata.encodings = [2, 4, 3]
    metadata.path_in_schema = [name]
    // must set the number as a string,
    // because parquet does not check null properly
    // and will think the value is not provided if
    // it is falsey (includes zero)

    metadata.codec = '0'
    metadata.num_values = count
    metadata.total_uncompressed_size = new Int64(data_page.length)
    metadata.total_compressed_size = new Int64(data_page.length)
    metadata.data_page_offset = new Int64(start) //just after PAR1

    return column
  })
  //the name "row group" suggests that a row group
  //should contain a column chunk for every row.
  //basically, we stream the input out chunks, a row group at a time.
  //these can be streamed to a file... we just save the file metadata
  //to be written at the end.

  var row_group = new types.RowGroup()
  //row group has
  // - columns
  // - total_byte_size
  // - num_rows
  // - sorting_columns

  // with multiple columns, these will be one after another obviously.
  // for the first data_page, file_offset will be 4.
  // starts just after the "PAR1" magic number.

  row_group.columns = column_chunks
  row_group.num_rows = count
  row_group.total_byte_size = new Int64(buf.length - 4)

  fmd.version = 1
  fmd.schema = [_schema].concat(schemas)
  fmd.num_rows = count
  fmd.row_groups = [row_group]
  fmd.created_by = 'parquet.js@'+require('./package.json').version 

  var _output = encode(fmd)
  var len = new Buffer(4)
  len.writeUInt32LE(_output.length, len)

  buf.append(_output)
  buf.append(len)
  buf.append(PAR1)

  return buf.slice(0, buf.length) //copy the buffer
}

if(!module.parent)
  process.stdout.write(module.exports(['a', 'b'], [
    ['one', '10'],
    ['two', '20'],
    ['three', '30'],
    ['four', '40'],
    ['five', '50']
  ]
  ))

