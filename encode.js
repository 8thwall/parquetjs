var Int64 = require('node-int64')
var varint = require('varint')
//encode an empty parquet file.
//should be like this:
/*
PAR1
<FileMetadata>
<length(FileMetaData)>
PAR1
*/

var thrift = require('thrift')
var types = require('./gen-nodejs/parquet_types')

function plain(value) {
  var v = new Buffer(value)
  var len = new Buffer(4)
  len.writeUInt32LE(v.length, 0)
  return Buffer.concat([len, v])
}

//value must be a thrift type.
function encode(value) {
  var output = []
  var transport = new thrift.TBufferedTransport(null, function (buf) {
    output.push(buf)
  })
  var protocol = new thrift.TCompactProtocol(transport)
  value.write(protocol)
  transport.flush()
  return Buffer.concat(output)
}

function encodeRepeats(repeats, value) {
  var len = varint.encodingLength(repeats << 1)
  var b = new Buffer(4 + len + 1)
  b.writeUInt32LE(len+1, 0)
  varint.encode(repeats << 1, b, 4)
  b[4 + len] = value
  return b
}

function encodeColumn(column) {


}

module.exports = function (name, column) {
  var PAR1 = new Buffer("PAR1")

  var count = column.length

  var fmd = new types.FileMetaData()
  var _schema = new types.SchemaElement()
  _schema.name = 'hive_schema'
  _schema.num_children = 1

  var schema = new types.SchemaElement()
  schema.name = name
  schema.type = 6 //binary aka string
  schema.repetition_type = 1
  //note, javascript code generated by thrift does not check
  //falsey values correctly, but parquet uses an old version of thrift
  //so it's easier to set it like this.
  schema.converted_type = '0'

  var values =
    Buffer.concat([
      //these 6 bytes are actually a hybrid RLE, it seems of the repetition level?
      //the column starts with a hybrid-rle/bitpack of the definition
      //level. for a flat schema with all fields, that is the
      //same as a lot of 1's. that can be encoded most compactly
      //as a RLE.

      //Question: how is the bitwidth of the RLE calculated?
      //I'm guessing it's something in the schema?
      encodeRepeats(column.length, 1)
    ].concat(column.map(plain)))

  var ph = new types.PageHeader()

  ph.type = '0'
  ph.uncompressed_page_size = values.length
  ph.compressed_page_size = values.length
  ph.crc = null
  ph.data_page_header = new types.DataPageHeader()
  ph.data_page_header.num_values = count
  ph.data_page_header.encoding = '0'   //plain encoding
  ph.data_page_header.definition_level_encoding = 3 //3 //RLE encoding
  ph.data_page_header.repetition_level_encoding = 4 //Bitpacked encoding
  //statistics is optional, but leaving it off probably slows
  //some queries.
  //ph.data_page_header.statistics

  var data_page = Buffer.concat([
    encode(ph),
    values
  ])

  var row_group = new types.RowGroup()
  //row group has
  // - columns
  // - total_byte_size
  // - num_rows
  // - sorting_columns

  var column = new types.ColumnChunk()
  var metadata = new types.ColumnMetaData()

  var L = data_page.length //id_data.length

  row_group.columns = [column]
  row_group.num_rows = count
  row_group.total_byte_size = new Int64(L)


  column.file_offset = new Int64(4) //starts just after the PAR1 magic number.
  column.meta_data = metadata
  metadata.type = 6
  metadata.encodings = [2, 4, 3]
  metadata.path_in_schema = [columnName]
  //must set the number as a string, because parquet does not check null properly
  //and will think the value is not provided if it is falsey (includes zero)
  metadata.codec = '0'
  metadata.num_values = count
  metadata.total_uncompressed_size = new Int64(L)
  metadata.total_compressed_size = new Int64(L)
  metadata.data_page_offset = new Int64(4) //just after PAR1
  //metadata.statistics is apparently optional.

  var zero = new Buffer(8)
  zero.fill(0)
  fmd.version = 1
  fmd.schema = [_schema, schema]
  fmd.num_rows = count
  fmd.row_groups = [row_group]
//  fmd.key_value_metadata = []
  fmd.created_by = 'parquet.js@'+require('./package.json').version 

  var _output = encode(fmd)
  var len = new Buffer(4)
  len.writeUInt32LE(_output.length, len)

  return Buffer.concat([
    PAR1,
    data_page,
    _output,
    len,
    PAR1
  ])

}

if(!module.parent) process.stdout.write(module.exports('id', [
  'one', 'two', 'three', 'four', 'five',
  'one', 'two', 'three', 'four', 'five',
  'one', 'two', 'three', 'four', 'five',
  'one', 'two', 'three', 'four', 'five',
  'one', 'two', 'three', 'four', 'five',
  'one', 'two', 'three', 'four', 'five',
  'one', 'two', 'three', 'four', 'five',
  'one', 'two', 'three', 'four', 'five',
  'one', 'two', 'three', 'four', 'five',
  'one', 'two', 'three', 'four', 'five',
  'one', 'two', 'three', 'four', 'five',
  'one', 'two', 'three', 'four', 'five',
  'one', 'two', 'three', 'four', 'five',
  'one', 'two', 'three', 'four', 'five',
  'one', 'two', 'three', 'four', 'five',
  'one', 'two', 'three', 'four', 'five',
  'one', 'two', 'three', 'four', 'five',
  'one', 'two', 'three', 'four', 'five',
  'one', 'two', 'three', 'four', 'five',
  'one', 'two', 'three', 'four', 'five',
  'one', 'two', 'three', 'four', 'five',
  'one', 'two', 'three', 'four', 'five',
  'one', 'two', 'three', 'four', 'five',
  'one', 'two', 'three', 'four', 'five',
  'one', 'two', 'three', 'four', 'five',
  'one', 'two', 'three', 'four', 'five',
  'one', 'two', 'three', 'four', 'five',
  'one', 'two', 'three', 'four', 'five',
  'one', 'two', 'three', 'four', 'five',
  'one', 'two', 'three', 'four', 'five',
  'END'
]))


